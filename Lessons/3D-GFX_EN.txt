
   ************************************************************************
 ****************************************************************************
************                                                      ************
**********            Techniques and application algorithms         **********
**********             for the creation of 3D graphics               **********
**********               engines with the computer                 **********
************                                                      ************
 ****************************************************************************
   ************************************************************************
   
Created by: -+- Cristiano Tagliamonte -+- Aceman/BSD -+-

Last revision: 29 September 1996


				              !     !
		        _..-/\        |\___/|        /\-.._
		     ./||||||\\.      |||||||      .//||||||\.
	  	  ./||||||||||\\|..   |||||||   ..|//||||||||||\.
	     ./||||||||||||||\||||||||||||||||||||/|||||||||||||\.
	   ./|||||||||||||||||||||||||||||||||||||||||||||||||||||\.
	  /|||||||||||||||||||||||||||||||||||||||||||||||||||||||||\
	 '|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||`
	'||||'     `|||||/'   ``\|||||||||||||/''   `\||||||'     `|||`
	|/'          `\|/         \!|||||||!/         \|/'          `\|
	V              V            \|||||/            V              V
	`              `             \|||/             '              '
				                  \./
				                   V


CONTENTS				
==========================================================

° Introduction
° Vectors
° Perspective
° Rotation
° Rotation optimisation
° Wireframe
° Hidden face
° Filled vector and scan line
° Flat shading
° Optimisations for light source calculation
° Gouraud shading
° Phong shading
° Reflection mapping
° Texture mapping
° Free direction texture mapping
° Bilinear texture mapping
° Biquadratic texture mapping
° Bump mapping
° 2D clipping
° Fill optimisation
° Appendix A: Fixed-point notation
° Appendix B: Polar coordinates
° Appendix C: Object management
° Final notes


INTRODUCTION
==========================================================

This short text is intended to help those who want to embark on the arduous
and fascinating journey towards programming a 3D graphics engine,
initially discovering the basic concepts and then tackling the more
complex and spectacular effects that can be achieved. The term ‘engine’
refers to the set of routines used to manage and manipulate
specific data which, once processed in the proper manner, will result in
the visualisation of the3D environment in real time.
The following text is not intended to be a programming course geared
towards 3D applications, but rather a work that provides
a simple explanation of the concepts on which many 3D engines are based.
For a complete understanding, basic knowledge of
trigonometry, linear algebra and a programming language
(preferably not advanced) is recommended.


VECTORS
==========================================================

A vector is simply a quantity with a direction and
a sense, or in simple terms, a line. In this context,
vectors will always be specified as going from a point with coordinates (0,0,0) (the origin
of the axes) to another point (x,y,z), thus allowing us to say
that this vector has a quantity of (x,y,z).
_
| /| A vector is indicated by a letter, for
| / example in the figure on the right,
(0,0,0)|/ x represents the vector V(x,y,z). The reference system
----+------------> used is given by three variables, which can be imagined
/|\ as a simple Cartesian plane perpendicular
z / | \ to the z-axis. For simplicity, let us consider the
/ | \ growth of y downwards (and not upwards
| \) in order to reduce the calculations to be performed
| \. V for the display of a point on the screen.
y| The z-axis grows when it moves away
v from the observer (who is stationary), while
it decreases when it approaches the observer.
We will use vectors to define each point in space. This does not
mean in practical terms that a vector constitutes a line
displayed on the screen, but rather indicates the imaginary segment between
the origin of the axes and a point in space.


PERSPECTIVE
==========================================================================

One of the very first problems that arise when creating a
3D engine is how to display a point in space on the screen. In fact, each
point has three coordinates, while we only have two available on the monitor
: the z-axis is missing! To solve this,
all we need to do is calculate the projection of the points onto the plane
coinciding with the screen, which is actually not complicated at all.
Let's imagine that our monitor is transparent, i.e. that we can
see what is inside it, and suppose that there is a
cube jumping around inside. This is the image we would see on our screen:
________________________________
| |
| |
| ____ |
| / /\ |
screen --> | /___/ \ |
| \ \ / |
| \___\/ |
| |
| |
|________________________________|

Let's assume that our gaze coincides with the z-axis and that the centre
of the monitor coincides with the origin of the three axes:

+ monitor +----------+ monitor
z-axis | _ | |
<---------|- - - - - ¢_> | . |
| eye | (0,0,0) |
+ +----------+

Now the situation we find ourselves in, viewed from the side, is as follows:

(cube)
___________________ A
| |-_ + (screen)
| | -_ |
a | | -_ |
s | | -_|A'
s | | +_
e | | | -_
| | | -_
Y | | | -_
| | | -_ _
|___________________|B_______|B'_______- ¢_> (eye)
(0,0,0)
<---- Z axis ---->

eye = our point of view (let's call it O, it is not the origin!)
A,B = points in xyz space
A',B' = points projected onto the monitor (B' coincides with the origin)
AB = segment equal to the y-coordinate of A in three dimensions
A'B' = segment equal to the y-coordinate of A' (because that of B' = 0)
^^^^^^^^^^^^^^^^^^ y projected onto the monitor!
BO = distance observer-point
B'O = distance observer-screen (let's call it d)

The coordinates on the monitor represent exactly those of the projected points,
 which we must therefore calculate. It can be seen that the right-angled triangles
AOB and A'OB' are similar in that they have an angle in common
and both have a right angle, so we can write the following
proportion:
A'B'/AB=B'O/BO which is equal to...
A'B'=AB*B'O/BO

Since B' corresponds to the origin of the xyz axes, BB' is the z coordinate
of point A, therefore BO=BB'+B'O=z+d. It is possible to arbitrarily establish
the observer-screen position (it is advisable to use a value
equal to 256, so that it will be possible to avoid multiplications by performing
a left shift of 8 bits).
The projected y-coordinate (yp) defined as A'B' is therefore equal to:

yp=d*y/(z+d) because A'B'=yp AB=y B'O=d BO=z+d

We have thus calculated the y coordinate of our point on the screen!
The same applies to the x coordinate: just consider the
situation seen from above, where the cube is slightly shifted to the right:

(cube)
___________________ A
| |-_ |(screen)
| | -_ |
a | | -_ |
s | | -_|A'
s | | +_
e | | | -_
| | | -_
X | | | -_
| | | -_ _
|___________________|B_______|B'_______- ¢_> (eye)
(0,0,0)
<---- Z axis ---->
xp=d*x/(z+d)

To clarify the differences between the top view and the side view, here are two
images of the same model. The first figure is simply
the Enterprise (the spaceship from the film Star Trek) seen from the side, while
the second is seen from above.


\==================================| _=_
\_________________________________/ ___/==+++==\___
‘’‘\__ \’“” |======================================/
\__ \_ / .. . _/--===+_____+===--‘’
\__ \ _/. .. _/ `+'
USS ENTERPRISE \__ \ __/_______/ \ /
NCC-1701 ___-\_\-'---==+____| ---==O=-
__--+‘ . . . ’==_ / \
/ |. . .. -------- | \
‘==+_ . . -------- | / side view
’‘\___ . .. __==’
‘’“”--=--‘’

_____
_.-“ `-._
.-” ` || || “ `-.
_______________ _ ,” \\ // `.
/ || \ /“ \ _,-----._ / \
|_______________||_/ / \\ ,” \ | | / `. // \
| | _] \ / \ ,---. / \ // \
| \__,--------/\ ` | \ / \ / |/ - |
) ,-“ _,-” |- |\-._ | .---, | -| == |
|| /_____,---“ || |_|= || `-”,--. \| -| - ==|
|:(==========o=====_|- || ( O )|| -| - --|
|| \~~~~~`---._|| | |= || _,-.`--“ /| -| - ==|)
 `-.__ `-. |- |/-” | `---“ | -| == |
| / `--------\/ , | / \ / \ |\ - |
__|____|_______ _ ] / \ / `---” \ / \\ /
| || \ \ // `._/ | | \_.“ \\ /
\_______________||_/ \ / `-----” \ /
`. // \\ ,“ viewed from above
`-._ || || _,-”
`-._____,-'


It should be noted that in the video, the coordinates originate from the
top left point, while for the 3D->2D transformation, they were
considered at the centre of the monitor. To overcome this problem, simply
add a constant to xp and yp at the end of the projection calculations,
 which will shift the relative axes by a number of
pixels equivalent to the constant. In summary, setting d=256, the coordinates
projected onto the screen (from A(x,y,z) to A'(xp,yp)) are equivalent to:

cx = screen width/2, to translate the x-coordinate to the right
cy = screen height/2, to translate the y-coordinate downwards

xp=256*x/(z+256)+cx
yp=256*y/(z+256)+cy

Please note that the z coordinate of the point cannot coincide with
the viewpoint, as this would result in a division by zero.
Nor should the depth of a point be set lower than that of the
viewpoint: it would be absurd to be able to see points behind
the observer!


ROTATION
==========================================================

We will only deal with rotations around the x, y and z axes. Other rotation formulas
around an arbitrary axis can be derived from
those we will see.
First, let's explain the meaning of rotation around an axis.
Consider an analogue clock, whose hands rotate around the axis
(passing through the centre of the clock) perpendicular to the clock itself:

\
\ ______________
X____________ /
//\ 12 // clock
// \ / //
//9 \/___ 3//
// //
// //
// 6 //
//___________/X
/_____________/ \ imaginary axis around which
\ the hands rotate

Technically speaking, we refer to the rotation of a point on an axis when the
point moves on the plane belonging to the point itself and perpendicular
to the axis so as not to alter the point-axis distance (which remains
constant). In this way, the point makes a circular movement around
the axis, i.e. it rotates around it, and the point-axis distance acts as the radius
for the circumference described by the rotation of the point in question.
Rotation around the y-axis can be imagined as the trajectory
travelled by a point that rotates around the y-axis without changing its
ordinate, which remains unchanged.

Now let's see how to actually perform rotations. Using a 2-dimensional plane as a reference system, it is possible to convert the
Cartesian coordinates (x, y) of a point into polar coordinates (r, t):

_ V V(x, y)=V'(r, t) _ V' r=distance
y| /| | /| point-origin
| / r=sqrt(x*x+y*y) | r/
| / t=arctan(y/x) | / t=angle
|/ |/) t between the vector
+-------> x=r*cos(t) +-------> and the positive x-axis
x y=r*sin(t)

Then, to rotate the point around the origin, you must add
the angle by which you want to rotate the point to the variable t and convert the
resulting polar coordinates to Cartesian coordinates. This method is too
slow for real-time applications as it involves squares,
arc tangents and square roots; Furthermore, introducing the variable z
would result in heavy polar coordinate management
(use of triple integrals, etc.): it is better to find a simpler
and faster solution.
Let's imagine we have a vector with zero ordinate V(x,0) and we want to
rotate it by an angle a:

y vector y _ Vr(xr,yr) vector
| coincident | /| rotated
| the abscissa | / of a
| | / radians
| V(x,0) |/) a
+----->-> +------->
x x

If we wanted to transform the coordinates of V into polar coordinates, we would get r=x
(x=sqr(x*x+0*0)) and t=0 (0=arctan(0/x). To rotate the vector, simply
add the angle of rotation a to t. Therefore:

(V = generic vector in Cartesian coordinates)
(V' = generic vector in polar coordinates)
(Vr = vector rotated by a radians in Cartesian coordinates)
(Vr' = vector rotated by a radians in polar coordinates)
(Vxr = vector rotated with only the x component in Cartesian coordinates)
(Vyr = vector rotated with only the y component in Cartesian coordinates)

V(x,y) = V'(r,t)
Vr(xr,yr) = Vr'(r,t+a) -> xr=r*cos(t+a) yr=r*sin(t+a)
Let's replace xr and yr with the relevant formulas:
Vr(r*cos(t+a),r*sin(t+a)) -> r=x t=0
Let's replace r with x and t with 0:
Vxr(x*cos(a),x*sin(a))

And this is the formula for the rotation of a vector that does not have
a y component. Now let's consider a vector that has zero abscissa V(0,y):

r=y (=sqr(0*0+y*y)) 
t=pi/2 (=arctan(y/0)=arctan(infinity))
Vyr(y*cos(pi/2+a),y*sin(pi/2+a))

A couple of trigonometric formulas tell us:

cos(pi/2+a)=-sin(a)
sin(pi/2+a)=cos(pi/2)

but since we are using a reference system with the y-axis ‘flipped’
we must change a sign in both formulas in order to use them,
which then become:

cos(pi/2+a)=sin(a)
sin(pi/2+a)=-cos(a)

Now the formula for rotating a vector without an x component is the same:

Vyr(y*sin(a),-y*cos(a))

But if we look at the general case, we have a vector V that has both
x and y components. In fact, a generic vector with x and y components
is equal to:

V1(x,0)+V2(0,y)=V(x+0,0+y)=V(x,y)

Now we can use the rotation formulas for the individual cases to calculate
the general case with an addition between vectors:

Vxr(x*cos(a) ,x*sin(a) ) +
Vyr( +y*sin(a), -y*cos(a)) =
------------------------------------------
Vr (x*cos(a)+y*sin(a),x*sin(a)-y*cos(a))

Thanks to this formula, it is possible to rotate any vector in a
two-dimensional space. In a 3D environment, the formula just described
coincides with rotation around the z-axis (no z-coordinate is
changed). To rotate the point around another axis, simply leave out
the relevant variable and use the others in the previous expression,
which can be summarised as follows:

around the z axis around the y axis around the x axis
-------------------- -------------------- --------------------
xr=x*cos(a)+y*sin(a) xr=x*cos(a)+z*sin(a) yr=y*cos(a)+z*sin(a)
yr=x*sin(a)-y*cos(a) zr=x*sin(a)-z*cos(a) zr=y*sin(a)-z*cos(a)


ROTATION OPTIMISATION
==========================================================================

Given a rotation angle for each axis (x, y and z) with the previous
formulas, 12 multiplications would be required to rotate a single
point. Here we will see how to perform rotations by performing 9 multiplications
for each point. Let us consider:

ax=angle of rotation around the x-axis s1=sin(ax) c1=cos(ax)
ay=angle of rotation around the y-axis s2=sin(ay) c2=cos(ay)
az=angle of rotation around the z-axis s3=sin(az) c3=cos(az)

Each of the variables x, y and z influences the rotations around two axes
(in the rotation around its own axis, the variable remains unchanged),
we can thus indicate with x' y' and z' the partially rotated variables
(i.e. after the first rotation) and with x'“ y”' and z'“ the
completely rotated variables. That said, the formulas seen above
correspond to:

x” = x*c1+y*s1
y' = x*s1-y*c1

x'“= x”*c2+z*s2 <- completely rotated x coordinate
z' = x'*s2-z*c2

y'“= y”*c3+z'*s3 <- completely rotated y coordinate
z'“= y”*s3-z'*c3 <- completely rotated z coordinate

which is equivalent to writing:

x'“= (x*c1+y*s1)*c2+z*s2=c2*c1 *x + c2*s1 *y + s2 *z

y”'= (x*s1-y*c1)*c3+((x*c1+y*s1)*s2-z*c2)*s3=
c3*s1 *x - c3*c1 *y + s3*s2*c1 *x + s3*s2*s1 *y - s3*c2 *z=
(s3*s2*c1+c3*s1) *x + (s3*s2*s1-c3*c1) *y + (-s3*c2) *z

z''= (x*s1-y*c1)*s3-((x*c1+y*s1)*s2-z*c2)*c3=
s3*s1 *x - s3*c1 *y - c3*s2*c1 *x - c3*s2*s1 *y + c3*c2 *z=
(-c3*s2*c1+s3*s1) *x + (-c3*s2*s1-c3*c1) *y + (c3*c2) *z

From the last step of each of these formulas, you can see that
partially rotated coordinates are not calculated and that each rotated coordinate
is equal to the sum of the (non-rotated) variables multiplied
by a certain factor. If we pre-calculate these factors, we can
use them for all points that need to be rotated in the same
direction. In this way, we will have simply performed 9 multiplications
for each point (excluding the precalculations for the factors).
Essentially, we must first calculate these constants:

xx=c2*c1
xy=c2*s1
xz=s2
yx=c3*s1+s3*s2*c1
yy=-c3*c1+s3*s2*s1
yz=-s3*c2
zx=s3*s1-c3*s2*c1=s2*c1+c3*s1
zy=-s3*c1-c3*s2*s1=c3*c1-s2*s1
zz=c3*c2

Then, for each point, the following calculations must be performed (using the
same factors):

x'“=xx * x + xy * y + xz * z
y”'=yx * x + yy * y + yz * z
z''=zx * x + zy * y + zz * z

And we will obtain the three rotated coordinates.
This algorithm would be less efficient than the previous one if
few points were used, while in the case of a large number of
vectors, significant savings in calculation time can be achieved.


WIREFRAME
==========================================================================

The wireframe is the simplest and oldest technique for reproducing
polygons. It simply consists of drawing lines that join the
vertices of the polygon to be represented, nothing else. The lines must be drawn
using the projected coordinates of the points (and not
those with three xyz variables).
Let's see how to draw lines if you are programming in a
language that does not allow you to perform this function directly
(such as C and Assembler). Let's analyse Bresenham's algorithm. We have
two points P1(x1,y1) and P2(x2,y2) and we want to display the line that
connects them:

P1(x1,y1)
.------______ ^
------______ P2(x2,y2) | dy
------______. v
dx
<------------------------------------>

consider:
x2 > x1
y2 > y1
dx = x2-x1
dy = y2-y1
dx > dy

All other types of lines can be derived from this type.
Then we calculate these values:

xl = x1 -> current x-coordinate of the point
yl = y1 -> current y-coordinate of the point
d = 2*dx-dy -> decision variable
d1 = 2*dy -> increment of d (if d<0)
d2 = 2*(dy-dx) -> increment of d (if d=>0)

Finally, we see the actual algorithm:

> loop of dx iterations
> display pixel at position (xl,yl)
> xl=xl+1
> if d<0 then:
> d=d+d1
> otherwise:
> d=d+d2
> yl=yl+1
> next iteration

A line is formed by a set of pixels. In our case, the number of
pixels that make up the line is equal to dx, so we need to create a
loop that repeats dx times, in which a point must be
displayed for each iteration. But what coordinates should this point have?
Let us indicate with xl and yl the coordinates of the point to be projected on the video,
which will initially coincide with those of P1(x1,y1). At the end of
each iteration, we increase xa, so that when we exit the
cycle, xa will coincide with x2 (because x2=x1+dx). What happens to the
ordinate of the pixel? We simply increment it when the variable d
is positive.
Another algorithm can also be used to draw lines, which
is often more efficient than Bresenham's (especially
when implemented in Assembly) and exploits the principle of linear interpolation.
 We will see this in more detail in the section on fill
and scan lines.


HIDDEN FACE
==========================================================

Hidden face means hidden face. In this section, we will see how to
eliminate it. In fact, in reality, in a non-transparent solid, not all faces are
visible for obvious reasons. Displaying only the visible faces on the monitor
is certainly more realistic than drawing them all.

The simplest and most intuitive algorithm is the “painter” algorithm. It consists
of sorting the faces that make up the object according to their z component.
The faces must then be drawn starting from the furthest
to the nearest; in this way, the faces drawn last
will be visible, while those visible will be drawn on the hidden ones
.
 To its detriment, this algorithm involves a significant waste of
machine time and is also practically unusable for wireframe graphics,
 but it allows any object (other than wireframes) to
be displayed correctly.

Another way is to calculate the normal (the perpendicular line)
on each face, check if it points towards the observer and, if
not, do not display it. This algorithm is only valid if the vertices
that delimit the face are stored in memory in a clockwise direction, as
the calculations we will see exploit this feature. Furthermore, the objects
must necessarily be convex, i.e. there must be no faces
that can “obscure” (not hide!) other faces.
The normal line can be considered a vector quantity that
in space is indicated as a common vector with three components.

The visibility of a polygon depends exclusively on its
orientation along the z-axis. To be more precise, we can say that
only the z component is necessary to know whether the face is hidden or not.
Let's consider our face as follows:

Three points are sufficient to identify a plane.
A(x1,y1) Consequently, to obtain the normal on the plane 
/\ coinciding with our face, the coordinates of the first three vertices of the face will suffice.
/ \ D / \ B(x2,y2)
Leaving aside any proofs, it is possible to state that the z component of the normal is equivalent to:
\ / C(x3,y3) (x2-x1)*(y3-y1)-(x3-x1)*(y2-y1)
\/
C(x3,y3) (x2-x1)*(y3-y1)-(x3-x1)*(y2-y1)

If the result is less than or equal to zero, the face is hidden,
otherwise it is visible. To simply find out whether this z component is
greater or less than zero, we could also use the projected coordinates
,
 i.e.:

(xp2-xp1)*(yp3-yp1)-(xp3-xp1)*(yp2-yp1)

The best way to visualise a concave object is to save the faces that are not hidden in a
buffer (checking the orientation of the
normal), then sort the faces according to their z component
and trace them from the furthest to the nearest.


FILLED VECTOR and SCAN LINE
==========================================================

Filled vectors are simply polygons “filled” with a
specific colour. Creating a fill routine means colouring the
content of a polygon by knowing the projected coordinates of its
vertices. Let's imagine that the polygon to be filled is the following:

| A|\ we could proceed as follows:
| | \ starting from the lowest y coordinate of the polygon
| | \ (in this case that of A) and gradually moving
| | \ to the last vertical position (i.e. y of D)
| | \ we colour the line delimited by the x positions
| | \ B of the sides in that y position.
| D \ |
| \ | The principle of filling is based on filling
| \ | horizontal lines starting from the top line
| \ | of the polygon to the bottom line.
|y \ |
v \|C Let's look at an example involving a single line:

| A|\
| | \ we need to fill the row shown in the figure.
| | \ Let's start from the x coordinate of side AD.
| row | \ Let's start by colouring this point.
| from -->|****\ Let's move on to the next point (i.e. the one to
| fill| \ B right) and colour it too; let's continue
| D \ | colouring the next pixels until we reach
| \ | to colour the point on the AB side and move on
| \ | to the next row.
| \ | This means that for each row we need the
|y \ | x coordinates of the extreme right point and the
v \|C extreme left point.

Now that we understand what to do, let's see how to do it.
We need to use two tables (one-dimensional matrices) in memory
with dimensions equivalent to the number of vertical pixels that can be represented on the
screen (e.g., in a 320*200 resolution, we need two tables of
200 values each). Let's consider each position in the tables as a
y position on the screen and the content of the first array as the
corresponding x component of the extreme point on the left, while the value
of the second array as the x component of the extreme right point.
By doing so, it will be sufficient to colour all the pixels in the row corresponding
to the position of the tables, starting from the x position contained in the
first table up to the x position contained in the second (all points
in a row have the same ordinate). Let's take a practical example:

0| x=0 ->A. <- x=0 for simplicity, let's consider segments AB and CD
1| x=0 -> .. <- x=1 inclined at 45 degrees. The vertices are:
2| x=0 -> . . <- x=2 A(0,0) B(5,5) C(5,10) D(0,5)
3| x=0 -> . . <- x=3 our two tables will be:
4| x=0 -> . . <- x=4 +-------------------------------------------+
5| x=0 ->D. .B<- x=5 |TAB1| 0| 0| 0| 0| 0| 0| 1| 2| 3| 4| 5|..|..|
6| x=1 -> . . <- x=5 +-------------------------------------------+
7| x=2 -> . . <- x=5 |TAB2| 0| 1| 2| 3| 4| 5| 5| 5| 5| 5| 5|..|..|
8| x=3 -> . . <- x=5 +-------------------------------------------+
9| x=4 -> .. <- x=5 To fill the polygon, simply colour the
10|y x=5 -> .C<- x=5 pixels between the corresponding values
v of the two tables using as the y component
the table index (which is the same for
both). Sometimes it is possible to eliminate the two extreme values of the
tables when there is only one pixel in those rows (as in our
example). Now all we have to do is extract the contents of these
two arrays.
The tables simply contain the x coordinates of all the points that
make up the sides of the polygon; these x-coordinates are also sorted
according to their y component. In practice, we have to perform a routine
to trace lines for all the sides of the face, in which we do not
display the pixels, but instead save the x component in an array whose
position is equivalent to the y of that point. This procedure is called
a “scan line”. In other words, given two points, the scan line represents the coordinates of all the points located along
the segment connecting the two known points. The procedure for creating a scan line for all the sides of a polygon is called “scan conversion” and basically means dividing the polygon into
the segment connecting the two known points. The procedure for
creating a scan line for all sides of a polygon is called
‘scan conversion’ and basically means dividing the polygon into a
set of rows and columns.
To create a scan line, you can use the
Bresenham algorithm, but it is better to use the linear interpolation process,
 which is more efficient. Let's take a quick look at what
this involves.
Consider two generic points A(x1,y1) and B(x2,y2) where y2>y1.
Now let's calculate:

dx=x2-x1 <-- length of the line connecting A and B
dy=y2-y1 <-- height of the line connecting A and B
stepx=dx/dy <-- number of horizontal pixels on each line

While the general algorithm is:

> x=x1
> y=y1
> loop of dy iterations
> if the y position of tab1 is free:
> save x in position y of tab1
> otherwise:
> save x in position y of tab2
> x=x+stepx
> y=y+1
> next iteration

This algorithm allows the calculation of a scan line in the case y2>y1, if
instead y1>y2, simply swap both coordinates of the two
points (i.e. consider y1 as y2 and x1 as x2).
Tab1 is the array containing the extreme points on the left, while tab2 contains the
extreme points on the right. With our algorithm, some
values written in tab1 may belong to tab2, and vice versa. Let's see
what to do to avoid this problem.

If we have the points saved in a clockwise direction, everything is simpler
and faster. Given two points A(x1,y1) and B(x2,y2) placed in a clockwise direction, if y1
is greater than y2, then the scan line belongs to tab1 (the one
containing the smaller x positions); otherwise, it will belong to tab2
(containing the larger x positions). Here is the complete algorithm for tracing
a scan line:

> compare y1 with y2
> if y1>y2:
> the correct tab is tab1
> if y1<y2:
> the correct tab is tab2
> swap y1 with y2
> swap x1 with x2
> if y1=y2: do not trace the scan line!
> dy=y1-y2
> dx=x1-x2
> stepx=dx/dy
> x=x2
> y=y2
> dy iteration loop
> save x in position y in the correct tab
> x=x+stepx
> y=y+1
> next iteration

Once the scan conversion of the polygon has been performed, we need to calculate the
lowest y component of the four points that make up the vertices of the
polygon and the height in pixels of the polygon itself. The lowest y coordinate
represents the index of the tables from which to start filling the polygon
and therefore the upper y position of the polygon. The height of the polygon is
equal to the difference between the largest y and the smallest y and is needed
to know how many rows we need to fill for the current polygon.

In summary, to fill a polygon, the following steps must be performed
:

- define two tables in memory sized to ys values (where ys
represents the height in pixels of the screen);
- calculate the lowest y of the vertices and the height of the polygon;
- obtain the scan line of each side of the polygon, saving it
in the appropriate table (scan conversion);
- starting from the lowest y position, fill the row delimited by the
x positions contained in the tables themselves for a number of times
equal to the height of the polygon.


FLAT SHADING
==========================================================

We have now reached the analysis of the first (and simplest) shading algorithm
, thanks to which we can assign a precise light intensity to each polygon comprising
the object, which will be determined
based on the orientation of the face with respect to the light source.
Flat shading allows each face to be assigned a single colour that
will determine how much the polygon is illuminated. Let's take an example:

+-------------+
view | |
from above | | <--- theoretical 3D object (theoretical
| | because in reality it does not exist, we
| | are going to visualise the
+-------------+ projected coordinates)
|
| <--- direction of the light source
______________|_____________ <--- monitor screen
|
|
or <--- observer's point of view

Let's assume that the light source corresponds to the point of view,
its direction is perpendicular to the screen. Let's define the angle of
inclination of the face with respect to the light source as the angle
between the line corresponding to the direction of the light and the
vector of the face (i.e. the normal line).
The smaller this angle, the more the polygon is oriented towards
the observer. We can intuitively understand that the more the face is facing
the observer, the greater the intensity of light applied to
that face will be. Consequently, a smaller angle will correspond to
greater brightness of the face. Here is another example:

/\
/ \ viewed from above
/ \
3D object --> / \
imaginary / \
/ \
\ /
\ / a = angle between the face vector
\ / and the direction of the light
/|\ /
/-| \ /
/ a| \/
vector / |
face --> / | <-- direction of light (in this case
/ | coinciding with the viewpoint)
/ |
-----------|------------- <-- monitor screen
or <-- observer's viewpoint

The light intensity attributable to the polygon is proportional to the cosine
of this angle. We know that the generic result of cos
(a) is between -1 and 1. Note that if the polygon is visible, our angle
varies from 0 to 90 degrees, otherwise the face is hidden (you can
use this feature to eliminate hidden faces!).
Therefore, the value of the cosine relative to our angle covers a range of
values from 0 to 1.
Furthermore, using 256 colours, it will be sufficient to multiply the cosine by 256
(or perform an 8-bit shift to the left) and we will obtain the chunky pixel
with which we will have to fill the relative face!
Let's see how to obtain this value.

First, we need to specify the colour palette to use.
This can be done by defining a palette in the video memory that starts
with the darkest colour and gradually gets lighter
.
To calculate the cosine, it is best to use Lambert's rule, which
states that the scalar product between two lines expressed as
vector quantities is equivalent to the product of the length of the
relative vectors and the cosine of the angle limited by the lines themselves,
i.e. the angle a. Therefore, to find cos(a), all we have to do is
calculate this scalar product and divide the result by the product
of the lengths of the two vectors.

To calculate the scalar product of two vectors, we multiply
the corresponding components and then add the results, for example:

H=(xh,yh,zh) ; K=(xk,yk,zk)
H*K=xh*xk+yh*yk+zh*zk <-- scalar product

To find the length of a vector, we can use Pythagoras' theorem,
 which states that the length is equal to
the square root of the sum of the squares of each component.

Let's check how to calculate the coefficients x, y and z of the face vector
:

Nx=(y2-y1)*(z3-z1)-(y3-y1)*(z2-z1) <-+--- the three coefficients
Ny=(z2-z1)*(x3-x1)-(z3-z1)*(x2-x1) <-| of the normal line
Nz=(x2-x1)*(y3-y1)-(x3-x1)*(y2-y1) <-+

N.B.: the points must be stored in memory in a clockwise direction!

x1, y1, z1 = components of the first point of the polygon
x1, y2, z2 = components of the second point of the polygon
x3, y3, z3 = components of the third point of the polygon

Finally, here is the formula for calculating the chunky pixel:

Nx*lx + Ny*ly + Nz*lz
cos(a)=-------------------------------------------------
sqrt(Nx*Nx+Ny*Ny+Nz*Nz) * sqrt(lx*lx+ly*ly+lz*lz)

chunky pixel = 256*cos(a)

a = angle between the unit vector and the direction of the light source
lx = x component of the light source
ly = y component of the light source
lz = z component of the light source

The coordinates lx, ly and lz represent the position of the light source.
 If the light coincides with the observer's point of view,
the relative coordinates will be:

lx=0 ; ly=0 ; lz=-256

zl is equal to the opposite of the distance between the observer and the screen
(in our case, the distance between the observer and the screen is 256).


OPTIMISATIONS FOR CALCULATING THE LIGHT SOURCE
==========================================================

In this section, we will see how to speed up our 3D engine if
it includes the implementation of a real light source.

A first optimisation consists of using a buffer where we will
pre-calculate all the normals of each face (or each vertex in the case
of Gouraud shading); then, instead of calculating all the normals at each frame,
 we rotate our pre-calculated unit vectors by the same angle
by which we rotate the vertices of the object using the exact same
procedure described above (preferably using the
9-multiplication algorithm).

We have said that the scalar product between the normal vector and the
vector corresponding to the light source multiplied by 256 allows us
to know the chunky pixel. Now let's see how to eliminate square roots
and division immediately. Let's analyse the formula again:

Nx*lx + Ny*ly + Nz*lz
cos(a)=-------------------------------------------------
sqrt(Nx*Nx+Ny*Ny+Nz*Nz) * sqrt(lx*lx+ly*ly+lz*lz)

To implement this optimisation, we must make the normal vector
and the vector corresponding to the light source unitary. Making a vector unitary
means dividing each of its components by its distance
from the origin; this ensures that the new components have a range
between -1 and +1, which is why we say they are unit vectors.
Let's see algebraically how to make any unit vector unit:

Nx
uNx=---------------------------
sqrt(Nx*Nx + Ny*Ny + Nz*Nz)

Ny
uNy=---------------------------
sqrt(Nx*Nx + Ny*Ny + Nz*Nz)

Nz
uNz=---------------------------
sqrt(Nx*Nx + Ny*Ny + Nz*Nz)

More generally, given a vector V(x,y,z), to calculate the
components of the relative unit vector uV(ux,uy,uz):

x
ux=---------------------
sqrt(x*x + y*y + z*z)

y
uy=---------------------
sqrt(x*x + y*y + z*z)

z
uz=---------------------
sqrt(x*x + y*y + z*z)

To make the light source unitary, we can also avoid
dividing each of its components by its length, since
we are the ones who decide where it is located, so we can arbitrarily
assign unitary coordinates. For example, returning to the case where the light
coincides with the point of view:

ulx=0 ; uly=0 ; ulz=-1

Therefore, the formula for calculating the light intensity is reduced to:

cos(a) = uNx*ulx + uNy*uly + uNz*ulz
pixel chunky = 256*cos(a)

Making a vector unit and then rotating it or rotating a vector and then
making it unit means performing the same function; therefore, when
we precalculate the normals, we can immediately make them unit,
then we will rotate the unit vectors. In this way,
 we avoid performing two square roots and one division per vertex at each
frame!

N.B.: if you want to move the light source, using this
optimisation you will not be able to translate the light source,
 only rotate it, as the origin-light distance must
remain constant.

The last optimisation consists of keeping the light source fixed at one point
light source fixed at a point, more precisely, it must always coincide with
the observer's point of view. Of course, we will use unit coordinates
for the unit vectors and the light. Let's look at the formula for calculating the
chunky pixel in this particular case:

chunky pixel = 256*( uNx*ulx + uNy*uly + uNz*ulz) =
= 256*( uNx*0 + uNy*0 + uNz*(-1))=
= -256*uNz

Now our chunky pixel depends only on uNz, so we can
precalculate -256*uNz for each vertex instead of just uNz, rotate it
and immediately use this value as the chunky pixel. This way
we avoid 3 multiplications and 2 additions. Furthermore, since we only need uNz
, we can easily avoid rotating uNx and uNy, saving
another 6 multiplications per face (or per vertex in the case of
Gouraud shading). In total, we save 9 multiplications and 2 additions per
face (or per vertex in Gouraud shading)!
Of course, in addition to -256*uNz, we will also have to precalculate uNx and uNy
multiplied by 256 (256*uNx, 256*uNy) which are needed to rotate -256*uNz.
Furthermore, if we invert our palette, we can use 256*uNz instead of
-256*uNz.


gOURAUD SHADING
==========================================================================

This shading algorithm allows you to shade the interior of each
polygon, unlike flat shading, which assigns a single
colour per face.

First, we need to calculate the vector of each vertex of the object
rather than each polygon. The components of the normal at the vertex are
equivalent to the arithmetic mean of the components of the normals of all
the faces touching that vertex. Let's take an example:

____ let V be a generic vertex of a cube belonging to
/f2 /\ faces f1, f2 and f3. Consider the normals of
/___/V \ these faces, let's call these vectors N1, N2, N3.
\f1 \f3/
\___\/ N1(Nx1,Ny1,Nz1) N2(Nx2,Ny2,Nz2) N3(Nx3,Ny3,Nz3)


Then the normal on V is equivalent to:

NV( (Nx1+Nx2+Nx3)/3, (Ny1+Ny2+Ny3)/3, (Nz1+Nz2+Nz3)/3 )

In this case, there are 3 faces belonging to V. Depending on the object
you want to use, the number of polygons belonging to a vertex
changes.

Once all the normals (preferably already unit normals) have been precalculated
on each edge, we will need to calculate the amount of light that falls on each
of the vertices, i.e. the chunky pixel, using the law already studied
in flat shading (perhaps taking advantage of any optimisations mentioned in the
previous paragraph).

Next, we perform the scan conversion (explained in the paragraph dedicated
to fill and scan lines) of all visible polygons.

Now we need to linearly interpolate the chunky pixels
belonging to the vertices of each face. In practice, we will need to perform a
simple scan conversion of the polygon using the chunky pixels instead
of the x coordinates of the vertices, that's all. Of course, this should only be done
if the face is visible.

All that remains is to perform the actual fill of the polygons. As
with a normal fill, we need to perform a loop with iterations equivalent
to the height in pixels of the polygon. At each iteration, we take
the initial and final x coordinates from the scan line tables
(as with a normal fill), but this time we also take the initial and final chunky pixels
.
Now we need to interpolate the initial chunky pixel with the final one,
 starting from the initial x coordinate and ending at the final one. To
do this, simply use the algorithm to draw a scan line with the
following modifications:

- use the initial chunky pixel instead of the x1 coordinate;
- use the final chunky pixel instead of the x2 coordinate;
- use the initial x instead of the y1 coordinate;
- use the final x instead of the y2 coordinate;
- use the chunky screen line to be “filled” as a table
where the scan line will be saved.

And here is the Gouraud shading!


pHONG SHADING
==========================================================================

Phong shading allows each pixel to be assigned its actual
light intensity, unlike Gouraud shading, which generates
shading within each face between the light intensities of each
vertex of the object.
The greater definition obtained with phong compared to gouraud
also involves a drastic increase in the operations that the
processor must perform. The heavy calculations required are such that
current processors are unable to render satisfactory scenes in
phong shading in real time.

In Gouraud, we calculate the actual light intensity at each vertex,
after which each colour is interpolated along each side of the polygon.
Finally, the colours on the extreme left sides are interpolated with the
colours on the extreme right sides in order to fill the entire
polygon.
In Phong shading, on the other hand, we always interpolate the normals; the colours are never interpolated.
 Once the normals have been determined on each vertex, they must
be interpolated along each side; then the normals on the
left-hand sides are interpolated with those on the
right-hand sides, and the colour is calculated for each individual vertex
using the traditional formula that has been studied many times.

Phong prevents us from exploiting the various optimisations possible
with Gouraud shading and flat shading. In fact, in Phong it is impossible
to use unit normals because when these are
interpolated, their length (equivalent to the result of the expression
sqrt(Nx*Nx+Ny*Ny+Nz*Nz)) can vary. Therefore, it is necessary to perform at least
one division and one square root per pixel, which is not insignificant.


REFLECTION MAPPING
==========================================================================

If a solid always reflects only a single image (in jargon
called a “texture”), then we can say that reflection mapping has been applied to that solid
.
If the texture roughly corresponds to the two-dimensional representation
of a light (e.g. a circle whose centre is very
light while the edges are shaded in a darker colour), it is
possible to achieve effects similar (and sometimes superior) to phong and
gouraud.

This effect is often mistakenly confused with environment
mapping, which instead allows you to reflect an entire environment
surrounding the object (which is often defined for simplicity
as a cube, so in this case six images will be reflected on the solid
).

This section will describe how to achieve reflection
mapping using only 256*256 pixel textures.
The implementation of different texture sizes is easily
derivable.

Let's take a detailed look at how to create a common object in reflection
mapping.
First, we pre-calculate all the unit vectors on each
vertex (as for Gouraud) and multiply them by 128 (or
apply a simple 7-bit shift to the left), which
mathematically translates to:
_ _
| PVx = 128*Nx / sqrt(Nx*Nx + Ny*Ny + Nz*Nz) |
PV | PVy = 128*Ny / sqrt(Nx*Nx + Ny*Ny + Nz*Nz) |
|_ PVz = 128*Nz / sqrt(Nx*Nx + Ny*Ny + Nz*Nz) _|

Let's call PV the vector that has these 3 values as components.
The unit normal has 3 values as coordinates, which include real numbers
between -1 and +1. Now we have PVx, PVy and PVz, which, compared to the unit vectors,
 are multiplied by 128, meaning that they will cover a range
of values between -128 and +128 (even if, in reality, these values
never exceed +127). This concludes the pre-calculation phase.

In real time, we must rotate the PV vector for each vertex (if necessary
using the same rotation factors as the points if
the rotation was performed with 9 multiplications). We only need the rotated x and y
components of the PV vector, so we can avoid rotating PVz, thus
avoiding at least 3 multiplications and 2 additions per vertex
(of course, it is necessary to precalculate PVz for each vertex in order to
rotate PVx and PVy). To each of the rotated x and y components of the PV vector
PV, we add the value 128. At the end of these calculations, the range of
values that PVx and PVy can take will be between 0 and 255.

In reality, ((rotated PVx)+128) and ((rotated PVy)+128) represent the
coordinates of the texture to be mapped (i.e. traced) on the polygon. This
means that if we have a polygon delimited by 4 points, we must
map onto that polygon the part of the texture delimited by the 4 relative PVx
and PVy rotated (and added to 128). Then, simply map the ‘piece’ of
texture onto that polygon and repeat the whole process for each visible face to
create reflection mapping!

Now let's see how to trace the part of the texture once the
new PVx and PVy have been calculated.
First, we need to perform the scan conversion of the polygon. In addition,
 we need to interpolate PVx and PVy along all sides of our face, which
means performing two additional scan conversions of the polygon using
the PVx and PVy instead of the x coordinates of the vertices. So, in total,
 there are three scan conversions: the first is the traditional one, the second
is done by replacing PVx with the x coordinates of the vertices, while the third uses
PVy instead of x (we do exactly as for normals in Phong,
with the difference that we consider two components (PVx and PVy) instead of three
(Nx, Ny and Nz)).
We have performed the scan conversion of the polygon. What we
need is an algorithm that allows us to associate each point
belonging to the face with a specific pixel of the texture.
Let's consider the following figure as our face projected on the screen:.

 After interpolating PVx and PVy and performing.
 . the scan conversion, for each pair of.
 . points on the same y position of the
P1 -> . . <- P2 on the screen, for example P1 and P2, we know.
 . their x coordinate together with PVx and PVy..
 . Now we need to interpolate PVx and PVy.
 . from point P1 to point P2, in this way.
 . we will know the value of PVx and PVy for all
. the points of the polygon. To interpolate
these two values along a line,
P1 -> x1, y, PVx1, PVy1 apply the general algorithm for
P2 -> x2, y, PVx2, PVy2 tracing a scan line using
dPVx = PVx1-PVx2 dx instead of dy, dPVx (to interpolate
dPVy = PVy1-PVy2 PVx) and dPVy (to interpolate PVy) instead of
dx = x1-x2 of dx, just as we did in
Gouraud to interpolate chunky pixels.

As already mentioned, PVx and PVy represent the coordinates of the texture pixel
to be traced. Once the three scan conversions were completed, we knew PVx and
PVy belonging to each vertex of the face. Therefore, by interpolating PVx and
PVy along the entire polygon, we will obtain the coordinates of the texture points
for all points of the face! Now, with simple copy operations,
 we can map the polygon by associating each of its points with the
chunky pixel of the texture in position (PVx,PVy).


tEXTURE mAPPING
==========================================================================

This effect allows the tracing of an entire image onto a
polygon; in practice, it is as if we were “gluing” a
texture onto each face.
In this section, we will discuss texture mapping without perspective, which
is the fastest algorithm for mapping an image onto
a polygon, but at the same time also the least realistic.

Let's consider using polygons formed by four sides, where each edge of the
polygon coincides with an edge of the face. What we need to do
is trace the entire texture onto the polygon. In practice, it is as if we
had to perform reflection mapping knowing that the coordinates of the
texture to be mapped are always constant and coincide exactly with the 4
vertices of the texture itself. If the texture is 256*256 pixels, we will implement
a reflection mapping algorithm knowing that PV1(0,0), PV2(255,0),
PV3(255,255), PV4(0,255) are the same for each polygon. In other
words, we are going to interpolate the x and y coordinates of the texture along
the entire polygon, so that we know for each pixel belonging to the
face to be traced what the relative point of the texture is. That's all there is to it.


fREE dIRECTION tEXTURE mAPPING 
==========================================================================


tEXTURE mAPPING bILINEARE
==========================================================================


tEXTURE mAPPING bIQUADRATICO
==========================================================================


bUMP mAPPING
==========================================================================


cLIPPING 2d
==========================================================================


FILL OPTIMISATION
==========================================================================


APPENDIX A: FIXED-COMMA NOTATION
==========================================================================

When processing 3D objects, you often have to deal with real numbers,
not integers. Most advanced languages allow direct
manipulation of such numbers, either by using a
mathematical coprocessor or by emulating them via software. The emulation
provided by current compilers is decidedly slow for
real-time applications. Furthermore, when working in Assembly,
direct management of real numbers is not available unless
the mathematical coprocessor is used. As an alternative to the FPU (which
uses floating point numbers), you can opt for the fixed point
format which, despite being less accurate than the floating point format,
 remains the best choice as operations performed in this
format are faster.
In principle, in an electronic computer, all numbers
(including real numbers) are represented as integers. Fixed-point notation
is based on the direct simplification of this
representation. Let's see how.

A real number is represented as the integer value given by the
product of the real number multiplied by a constant defined in advance.
It is precisely this constant that determines the precision with which
non-integer numbers can be represented. Here is an example:

3.25 <- real number
256 <- constant

3.25*256 = 832 <- 3.25 in fixed point

In this way, we can represent all real numbers with a reasonable
margin of error, which is practically insignificant for our applications.
 It is convenient to use a power of 2 as a constant
(e.g. 256, 65536), which speeds up the manipulation
of numbers in this notation. In fact, it is well known that computers represent
any number as a sequence of bits, so by using
a power of 2 as a constant, it is possible to define 2 bit fields for each
digit: one dedicated to the integer part and the other dedicated to the
fractional part.
If a fixed-point number has a number of bits dedicated to the integer part
integer part is equal to <a> and a number of bits dedicated to the fractional part
is equal to <b>, then that number is said to be in the format ‘a:b’. It should
also be specified that the integer part of a fixed-point digit
belongs to the highest bit field, while the fractional part
belongs to the lowest bit field.

3.25 <- real number
256=2^8 <- constant
832 <- 3.25 in fixed point
8:8 <- fixed point number format. If
we use 1 word (16 bits), we have the 8 most significant bits
dedicated to the integer part and
the other 8 least significant bits for the
fractional part.

Let's see how to convert an integer to fixed point format and
vice versa:

integer = (fixed point number) / (constant)
fixed point number = (integer) * (constant)

Finally, let's understand how to perform the 4 operations with
such numbers:

(a:b) + (c:d) = impossible!!
(a:b) + (a:b) = a:b
(a:b) * (c:d) = (a+c):(b+d)
(a:b) / (c:d) = (a-c):(b-d)

We immediately realise that it is impossible to add two fixed-point numbers
of different formats; we must first make the two digits homogeneous
(meaning that the two digits must have the same format). Note that
any integer can be understood as a fixed-point number
in the format ‘a:0’; therefore, it is possible to directly perform
multiplication and division between fixed-point numbers and integers.


APPENDIX b: POLAR COORDINATES
==========================================================

As we already know, to represent a generic point on a plane
we can use the Cartesian axes. The x and y components
represent nothing more than the projections of our point on the abscissa and
ordinate axes.
Let us imagine instead that we want to indicate a point using another
reference system, in our case polar coordinates.

^ Let's consider r as the distance between point P and
y | .P(x,y) the origin, and t as the angle between the
| / segment OP and the positive x-axis.
| / It is possible to indicate any point using
| r/ these two variables (r and t), which represent
| / the polar coordinates.
|/) t For each P(x,y) there is a corresponding P'(r,t). Let's see
+------------> how to perform these conversions.
O x
Let R(x,0) be the projection of P(x,y) onto the x-axis
^ (i.e. the point with ordinate 0 and equivalent
y | .P(x,y) abscissa of P). The triangle ORP is a right-angled triangle in R,
| /| therefore, by Pythagoras' theorem:
| / |
| r/ | r = OP = sqrt( x*x + y*y )
| / |
|/) t | We can also state that:
+-----+------>
O R x PR = r*sin(t) => sin(t) = PR/r
OR = r*cos(t) => cos(t) = OR/r

If we pay attention, we can state that (considering the point
P(x,y)), PR=y and OR=x. The tangent of an angle is equivalent to the ratio
of the sine of that angle to its cosine, therefore:

tan(t) = sin(t)/cos(t) = (PR/r)/(OR/r) = PR/OR = x/y
x/y = tan(t)

t = arctan(x/y)
r = sqrt(x*x+y*y)

x = r*cos(t)
y = r*sin(t)

Now we know how to convert Cartesian coordinates to polar coordinates and
vice versa, which will be useful for understanding how to rotate a point.
APPENDIX c: OBJECT MANAGEMENT


APPENDIX C: OBJECT MANAGEMENT
==========================================================

We want to draw our object on the screen, whether it is in wireframe,
gouraud shading, texture mapping or any other rendering technique that
suits us best; we know perfectly well how to display a single
face, but how can we manage all the faces that make up the three-dimensional solid
?
We need to define a “format” in which the object resides in memory,
based on which we can display any 3D figure using
the same routines that make up our 3D engine. Let's start with a
practical example:

V5 _____________ V6 Let's keep in mind that we need to define how
/|	 /| object a simple cube, first
/ |     / | we could indicate the number of
/ |     / | vertices and faces it is composed of;
/ |     / | then we list all the
V1 /____|_______ /V2 | coordinates (x,y,z) of the vertices of the cube;
| | | | finally, we indicate the characteristics of
| V8|_______|_____|V7 all the faces. In the simplest case
| / | / to define a face, it is sufficient to indicate
| / | / its vertices (ordered in clockwise order
| / | / to facilitate the removal
| / | / of the hidden face and the calculation of the
|/____________|/ normal). Here is how it is possible to
V4 V3 define a cube in memory:

8 <- number of vertices of the object
6 <- number of faces of the object
-50,-50,-50 <- x,y,z coordinates of vertex V1
+50,-50,-50 <- x,y,z coordinates of vertex V2
+50,+50,-50 <- x,y,z coordinates of vertex V3
-50,+50,-50 <- x,y,z coordinates of vertex V4
-50,-50,+50 <- x,y,z coordinates of vertex V5
+50,-50,+50 <- x,y,z coordinates of vertex V6
+50,+50,+50 <- x,y,z coordinates of vertex V7
-50,+50,+50 <- x,y,z coordinates of vertex V8
1,2,3,4 <- pointers to the vertices that make up face 1
2,6,7,3 <- pointers to the vertices that make up face 2
6,5,8,7 <- pointers to the vertices that make up face 3
5,1,4,8 <- pointers to the vertices that make up face 4
5,6,2,1 <- pointers to the vertices that make up face 5
4,3,7,8 <- pointers to the vertices that make up face 6

Let's analyse how polygons are defined, taking face 1:

1,2,3,4 <- means that the face is composed of the first 4
points in the list of vertices, namely:

-50,-50,-50 <- x, y, z coordinates of vertex V1
+50,-50,-50 <- x, y, z coordinates of vertex V2
+50,+50,-50 <- x, y, z coordinates of vertex V3
-50,+50,-50 <- x, y, z coordinates of vertex V4

Each side is represented by the linear conjunction of 2 vertices.
In the example just given, the sides of face 1 are the segments
delimited by the first and second points (V1 and V2), the second and third (V2
and V3), the third and fourth (V3 and V4) and the fourth and first points (V4 and
V1).
In our case, each face is a quadrilateral, but it is of course possible to
create your own 3D engine that uses a different number of sides.
The important thing is that each polygon is convex, otherwise the
scan conversion algorithm would be much more complex than the one studied
in the section on fill and scan lines.


FINAL NOTES
==========================================================

First of all, I would like to thank Randy/RamJam (Fabio Ciucci), from whom I
acquired a solid foundation in 680x0 Assembly and learned the knowledge
necessary to take advantage of the legendary Amiga's ECS/AGA custom chips.
I am also very grateful to Hedgehog/??? (Marco
Ricci), who provided me with a wealth of information (which enabled me to
write this document) and useful advice on coding techniques.
Thanks also go to Psyko/Vajrayana (Pasquale Mauriello), who
kindly sent me printed material to study and verify
part of my knowledge.
I salute all the Italian sceners who are working hard
to create something, whether it be good or bad:
the important thing is that the results are the fruit of their own labour.

If anyone is interested in receiving the latest version
of this text file free of charge by email or in reporting any
errors or inaccuracies, please contact me by email:

dayco@rgn.it

or write to me by post:

Cristiano Tagliamonte
Via Filippo Masci, 86/G
66100 Chieti


______ ____ _ _ ___ _
/ || __|| __||\ /| / ||\ ||
//|||| ||_ | \/ | //||| \||
/__ |||__ ||__ ||\/|| /__ |||\ |
// |||___||___||| ||// |||| \|
========================aCEmAN/bSd
