
ASSEMBLER COURSE - LESSON 14

- FUNDAMENTALS OF ACOUSTICS AND DIGITAL AUDIO -

(Source Directory8) - then write ‘V Assembler3:sorgenti8’

___
_( )_
__( . . .)__
_( _ .. ._ . )_
( . _/(_____)\_ )
(_ // __ | __ \\ __)
(__( \/ o\ /o \/ )__)
( .\_\__/ \__/_/. )
\_/ (_. ._) \_/
/___( )___\
( | |\___/| | )
||__| | |__||
||::|__|__|::||
||:::::::::sc||
.||:::__|__:;:||
/|. __ __ .|\.
./(__..| . .|.__) \.
(______|. .. |______)
/| |_____|
/|\


Author: Alvise Spanò

As you all undoubtedly know, sound is nothing more than a WAVE, or,
according to the physical definition, the “propagation of a disturbance in a
medium”; in the case of the sounds we are used to hearing, the disturbance
(wave) is initially emitted by the vibration of matter (molecules and/or
atoms), and the medium is air (a wave is an oscillation, a continuous exchange
of kinetic and potential energy, therefore mechanical, between molecules and/or atoms
(physically, it is the variation in pressure between particles of matter), and
therefore NEEDS matter to exist and spread: in a vacuum, for example,
 no sound waves or any other type of vibration propagate
between two distinct and separate bodies (only electromagnetic radiation
- at least so far, the only ones known - can
move even in a vacuum thanks to their dual physical nature as both
particles with mass - albeit very small - (photons) and waves).
The physical model adopted to describe this continuous transfer of energy
from one point to another in the medium is represented by the graph of the trigonometric function
SINE (= sen = sin), i.e. a SINUSOID.

>>> WAVE FUNCTION: y = f(x ± v*t) <<<

x e.g.: HARMONIC WAVE EQUATION: y = asink(x±vt) = a * sin(k * (x ± v * t))

- y = DEPENDENT VARIABLE: in a two-dimensional Cartesian graph
(x,y), the ordinate y represents
the “quotes” of each point x of the oscillation.

- a = WAVE AMPLIFICATION COEFFICIENT: as
you know, -1 <= sin(x) <= 1 (sine of x (x =
any real number) between -1 and 1, extremes
included), therefore, to obtain an oscillation that
varies from -a to a (-a <= sin(x) <= a), it is necessary to
multiply sin(x) by a real number a.

- k = WAVE FREQUENCY: by varying this parameter
you vary the frequency, and, inversely
proportional, the PERIOD of the wave, i.e.
the minimum interval along the axis of the independent variable
(in this case x) for which the sine wave
is cyclical, i.e. the minimum interval after
which the wave assumes the same characteristics.
At this point, we introduce the concept of WAVELENGTH
(= space travelled in the period = distance between
two adjacent cycle crests), and we explore
that of frequency:
 number of times that the same characteristics are assumed by the wave in a
given unit of time, i.e. how many times per second
the period is read (= cycles per second);
usually, the minute second [s] is considered as the unit
of time and the Hertz [Hz = s^-1 = 1/s] as the unit
of measurement of frequency.

- x = INDEPENDENT VARIABLE: in a two-dimensional Cartesian graph
(x, y), the abscissa x represents a
point along a straight spatial dimension
pre-established at a given instant; x belongs
to the real numbers and, theoretically, has no limitations,
i.e. it encompasses the entire straight line, causing a sinusoid to appear in the plane
considered (x, y) that continues
infinitely to the left and right
of the origin of the axes O(0,0): this makes it easy to
understand how the sine function has a
periodicity.
For example, if k = 1, the period of the wave
is 360 degrees (= 2*¶ radians) and the frequency
is 1 Hz; if k = 2, the period is 180° (= ¶ rad) and
the frequency is 2 Hz; and so on.

- v = PROPAGATION SPEED: indicates the speed in
kinematic terms at which a point on the wave moves through space
.
Note that v = WAVELENGTH * FREQ.

- t = TIME INSTANT: keep in mind that v * t = s =
= space, and s is the distance between two
corresponding points of the same disturbance taken at
t different times, therefore, adding/subtracting s to/from
x results in propagation over time, a movement
of the wave in space.
It should be noted that with (x + s) the wave will move
to the left, and with (x - s) to the right along the
x-axis.

P.S.: I apologise for the hastiness of the explanations
and the absence of demonstrations, but I do not think
this is the right occasion to dwell
excessively on topics that do not directly concern
assembler and coding in
general; so please take this
the above explanations as they are and
don't worry too much if you don't fully understand
the physics of waves: you won't need it
in the end to insert music into a
game or demo.

N.B.: Keep in mind that x and y represent any two
characteristics of the propagation phenomenon.
In the case of sound waves, we will consider x and y
as two spatial dimensions that describe the wave on a
plane by examining a section of it.

· ·
: :
________¦ ¦________
\ | __ | /
__________________\_____ | _\/_ | _____/__________________
\____________________ / | \/\/ | \ ____________________/
\____________ \_|._ _.|_/ ____________/
\ ____ _)| \/ |(_ ____ /
\/ / \__¯`“¯__/ \ \/
/ / / \ \ \
\/ \ `····” / \/
\ /
\ /
\ /sYz
\/

Now we can apply what we have learned from the basics of physics
about propagation phenomena in acoustics, defining
the concept of HARMONIC: a sound - which, incidentally, does not exist in nature and
can only be reproduced with electronic instruments, such as computers - which
has the waveform (= graph (x,y) of the disturbance throughout 
its duration) of a sine wave.
We can say that the harmonic is the model of the BASIC SOUND, which, when combined with many
others, creates all NON-pure sounds.
Physics also distinguishes three qualities in a sound so that it can be
described:

1- PITCH: This distinguishes between PURE sounds (consisting of a
single harmonic - non-existent in nature) and NATURAL
sounds (consisting of several harmonics superimposed in the
same time interval - some of those that
exist in nature have thousands of harmonics
of different periods), and qualifies its frequency,
altering which the various notes are produced.

2- INTENSITY: This can be seen as a sort of “volume”
of sound and, in the case of harmonics, is directly
proportional to the amplification (absolute value
of the Y values of the peaks) of the sine wave.

3- TIMBRE: This qualifies the waveform of the sound
regardless of the previous two parameters, therefore
it essentially describes the musical instrument or,
more generally, what we use to distinguish
one sound from another regardless of its
pitch and intensity.

Let's leave aside the various formulas for calculating sound intensity and
pressure and intensity levels [deciBell = dB], which do not directly concern
directly to electronic music but only to acoustics as a branch of physics
that deals with the diffusion of sounds in the environment and the design of
devices capable of reproducing (speakers, etc.) or capturing (microphones,
etc.) sounds, and let's focus on pitch and timbre.

*** First of all, timbre, in itself, is a non-existent parameter:
computers do not distinguish between sounds, but convert the digital data stored in their memory
into analogue signals (which transfer electrical intensities and not bits)
according to a reading frequency and a given value of underamplification,
without caring about the difference between sound and noise - two concepts that only
we humans have introduced with different meanings and attributable to the aesthetic sense
***.
Therefore, timbre cannot really be considered a parameter - at least
at an electronic level - even though there are sophisticated algorithms for
identifying and comparing the timbre of different waveforms that
could be useful to those who want to program
voice recognition routines, for example. In any case, this is certainly not the place
to discuss such complex applications in the world of synthesis and
sound processing.

Let us therefore move on to the most important parameter, as far as we are concerned:
the pitch of sounds is directly linked, in the case of NATURAL sounds, to the
frequency of the fundamental harmonic (FUNDAMENTAL FREQUENCY) that distinguishes them,
and, in the case of PURE sounds, to the frequency of the SINGLE harmonic that constitutes them
constitutes them.
Let's take an example: we have a pure sound (harmonic) with a period (= duration) X;
it can be emitted at any frequency, depending only on the “READING SPEED”
of the sound by the emitter: for example, if the emitter “reads”
the harmonic twice per second throughout its period, the frequency will be equal
to 2 Hz; at a later stage, the problem of the acoustic diffusion
of the sound in the air also arises, but this is easily resolved: the speed
of sound in air is 380 m/s, and therefore, knowing that
VELOCITY = WAVELENGTH * FREQ., it is extremely simple to obtain the wavelength
, which coincides, in terms of spatial dimensions, with the period (which is
expressed in seconds).
At this point, a new parameter comes into play when dealing with sound-generating instruments
: the reading speed, or SPEED/SAMPLING FREQUENCY
.
Now, however, in order to continue, it is necessary to explain how electronic instruments
handle sound and how they process it before passing it on to the amplifier.

Through the use of a sampler (audio digitiser) and appropriate software,
 it is possible to convert sounds from a sound source
(microphone, CD, etc.) into numerical data (digital samples), each of which
describes the “share” of an interval (a “tiny slice” - to use
scientific terms -) along the x-axis of the graph (x, y) of the waveform.
 The more samples we “capture”, the more defined and closer to
physical reality the digital sound will be.
For example, if we have a natural sound (i.e., non-harmonic) lasting
2 seconds (which we assume to be the period, since it is not possible to
identify a minimum interval for which the wave cycles), and we are able to
obtain its fundamental frequency, it is sufficient to sample it in
memory with a DOUBLE sampling frequency (Nyquist's theorem, explained
later) to obtain a faithful and acoustically defined reproduction of the
sound; if, for example, the fundamental frequency is 2 kHz (= 2000 Hz),
we would need to record 2000 samples per second, for a total of 4000 samples
(2000 samples/s * 2 s = 4000 samples).
** Each sample occupies 8 bits (= 1 byte) in memory in the Amiga,
which uses 8-bit sound definition.
With 8-bit numbers, it is possible to describe Y values between -128
(= -(2^8)/2 = -2^(8-1) = -2^7) and 127 (= (2^8)/2-1 = 2^(8-1)-1 = 2^7-1 (zero
has a positive sign in binary: in fact, positive numbers (from 0 to 127) are
128 like negative numbers), including extremes, of each individual sample, for a total of
256 (= 2^8) expressible values **.

N.B.: it is important to note that the waveform oscillates between the I (+) and II
(-) quadrants, divided by the abscissa of quota 0; DO NOT consider -128 as
0 and +127 as 255: it is NOT possible to shift the wave to the I quadrant and
make everything positive, * the calculations would not add up either on the sound chip
or in any sound processing for special effects via
software *.

*** CONSIDER EACH SAMPLE AS AN 8-BIT BYTE WITH SIGN (= MSB = bit 7) ***

It is important to note that digital synthesis with a bit number greater
than 8 would offer superior sound quality at the same sampling frequency
.
For example, CD players read samples at 16 bits (= 2 bytes = 1 word), which
means that the range varies from -32768 (= -(2^16)/2 = -2^(16-1) =
= -2^15) and 32767 (= (2^16)/2-1 = 2^(16-1)-1 = 2^15-1), for a total of 65536
(= 2^16) expressible values: *** this does not mean, however, that the value 32767
(positive peak) of the CD corresponds to a greater share of that sample
than the value 127 of the same sample rendered at 8 bits: the sound output
will be the same, except that, for the same physical range, 16-bit synthesis offers
much more definition (basically, at 8 bits I have 256 numbers to express a
sound, between two physically constant positive and negative peaks, which
however, at 16 bits, is synthesised with a much higher range of values
(65536) and therefore WITH MORE PRECISION AND LESS APPROXIMATION OF THE QUOTAS).
In a sense, we can say that the 8 bits in the first example
correspond to the upper 8 bits (15:8) of the 16 bits in the second example, and the lower 8 bits
correspond to a sort of approximation after a fictitious comma placed
between the upper and lower bytes of the sample word ***.

Let's now return to the sampling frequency, citing a famous and
important - as well as complicated to demonstrate - theorem
which states that
 “THE FREQUENCY RESPONSE IS EQUAL TO HALF THE SAMPLING FREQUENCY” (Nyquist's theorem): essentially, this means that if we
sample at 10 kHz, only sounds with a frequency
less than or equal to 10/2 = 5 kHz will be reproduced faithfully (this explains the mysterious ' DOUBLE' written
above about the sampling frequency required to sample the
given sound, knowing its fundamental frequency).
It is essential to sample sounds at an adequate frequency so as not to hear
ALIASING, which cuts frequencies above half the sampling frequency
, rendering them with an unpleasant “distorted” effect.
** Although Paula (for the record, the actual name of the Amiga sound chip)
uses only 8-bit digital precision, it is possible to reproduce
equally good quality sounds by sampling at the right frequencies and avoiding
aliasing ***, although, however, it is not possible to achieve the quality of a
CD, which samples 16 bits at 44.1 kHz (44100 Hz) to obtain a frequency response
ranging from approximately 20 Hz to 22 kHz, which corresponds approximately to the range
of frequencies audible to the human ear (subjective: some people may
reach up to approximately 20 kHz)
* I would like to take this opportunity to mention that sounds with a frequency (fundamental,
implied) lower than 20 Hz are called INFRASOUNES, and those with a frequency
higher than 20-22 kHz are called ULTRASOUNES: both are NOT audible to humans *.
However, most natural sounds do not have a fundamental frequency
higher than 15-16 kHz; is it therefore sufficient to sample at 32 kHz at most
to faithfully reproduce almost all existing sounds?
Well, no! For a very simple reason: as explained above,
natural sounds are made up of many harmonics, among which
a fundamental one can be identified: it could also happen that the fundamental frequency
(which we usually use to calculate the sampling frequency) is actually the frequency of the harmonic with the shortest period
(therefore, with the highest frequency)
; in this case, all harmonics with a frequency
higher than the one we assume to be fundamental would be cut off and
reproduced with aliasing, significantly lowering the overall sound quality.
*** IT WOULD THEREFORE BE ADVISABLE TO SAMPLING AT TWICE THE FREQUENCY OF THE
HIGHEST HARMONIC FREQUENCY THAT COMPOSES THE NATURAL SOUND ***.
_______
_ _/ /_____\
_ __\ Oo /
_ ______\_-_/______
(_/__/ : __\_) '
__/ : \__
_ (_/____:____\_>
_ O/ _ _O/

Therefore, intensity distinguishes the “volume” of sound, and it *** is NOT
constant with respect to frequency: at very high or very low frequencies, it
needs to be amplified in order to be perceived with the same intensity as
sounds of medium frequency by the ear, which biological evolution
(as a result of habit) has clearly led us to hear
those most common in nature better ***; what distinguishes the pitch of a
sound? In purely musical terms, it is easy to say: NOTES.
As you undoubtedly know, musical notes form scales of 7 notes per OCTAVE,
each of which begins with the note DO (C, in Anglo-Saxon notation)
and ends with B (B, in Anglo-Saxon notation - A is A); at each
octave, the frequency doubles, so each C is twice the frequency
of the previous C (note that the increase in frequency is not
linear, but EXPONENTIAL to the power of 2).
Within the octave, the relationships between the notes of the scale are as follows:

DO RE MI FA SOL LA SI | (DO)
--------+-------+-------+-------+-------+-------+-------+-------|---+---------
1 9/8 5/4 4/3 3/2 5/3 15/8 | 2

If you need to sample sounds that will then be used as instruments
in music editing programmes (such as SoundTracker, NoiseTracker or
ProTracker), it would be sufficient to sample at twice the frequency
of the fundamental frequency of the sound, which, if sampling from a musical instrument,
 corresponds to the frequency of the note played, at least for practical purposes:
if, for example, we need a piano for the composition of a module,
we can sample the A3 (A of the third octave) at 880 Hz (A3 = 440 Hz) and
tell the tracker that the sampling frequency corresponds to A3,
it will then calculate the correct frequencies relative to the sampling frequency
based on the notes we place on the score with that
instrument.
Now, you will surely ask yourself: is sampling at 880 Hz enough to
avoid aliasing?
The answer is no. As we said before, you would need to sample at twice
the frequency of the highest harmonic to faithfully reproduce the
timbre of the piano, but it is very complicated to obtain this frequency (
not to say impossible).
What to do, then? ** Well, try and try again to sample at various frequencies
(honestly, much higher than 880 Hz) until you get an optimal reproduction
of the instrument at that note and communicate to the tracker the
sample reading frequency for that note **.
As you can see, therefore, the matter is more complicated in practice than in
theory!


After these inevitable (and, I hope, interesting) notes on digital audio,
I will move on to a more specific explanation of the sound hardware of the Original and
AA chips of the Amiga (the Paula is the only custom chip that has never undergone
any improvements since the release of the first Amiga (1985, for the record)).
The hardware has 4 DMA channels dedicated to the 4 voices of the sound chip; these 4
voices are totally independent and are grouped in pairs for each speaker, resulting in
voices 1+4 for the left channel and 2+3 for the right channel in stereo.
All 4 voices also have their own hardware registers:

AUDxLCH $dff0y0 = Location of data to be read (high word)
AUDxLCL $dff0y2 = Location of data to be read (low word)
AUDxLEN $dff0y4 = DMA length (in words)
AUDxPER $dff0y6 = Read sampling period
AUDxVOL $dff0y8 = Volume
AUDxDAT $dff0ya = Channel data (2 bytes = 2 samples at a time)

N.B.: For each “x”, replace with a number from 0 to 3, corresponding to the
desired item; for each “y”, replace with a hexadecimal number
from $a to $d relating to items 0 to 3.

AUDxLCH-AUDxLCL:These constitute the latch value, not the DMA pointer to the
data, therefore, once set, it does not increment as
for planes, sprites or blitter channels, but is
similar to the copper location registers, the value of which
is automatically reinserted into the internal pointer registers
when needed.
* Since these two 16-bit registers are adjacent, it is
convenient to set them with a single MOVE.L of the 68000 type:
MOVE.L #miosample,AUDxLCH *.
N.B.: from now on, with AUDxLC I will refer to the pair of
two registers, a sort of single 32-bit location register
.

AUDxLEN:    Expresses the word length of the sample to be played.
If, for example, we have a 500-byte sample in memory,
we need to set this register (of one of the 4 desired channels)
 to a value of 250.
N.B.: As with the blitter, writing 0 to this register will
read 128 kB of samples.

AUDxPER:    This register is used to specify the DMA read frequency
in a somewhat bizarre way - apparently - but which is
convenient and fast for the hardware: it must be set with the
SAMPLING PERIOD of each individual sound sample,
a value that expresses the time (in cycles of the system DMA CLOCK
= 3546895 Hz (PAL), 3579545 Hz (NTSC)) that the DMA must
wait (it works as a decrementer: -1 per clock cycle)
before transferring another sample.
Here is a formula to calculate the value to be entered in this
register, given the sampling frequency (which is much more
practical to manage): PER = CLOCK / freq. [Hz]
For example, if we need to sample a harmonic A3 - assuming
that we find a natural source... - with a frequency of 440 Hz,
and a period of 1 second, we must adopt a sampling frequency
of 880 Hz, so here is the sampling period
to be entered in the AUDxPER register to read the sample in memory at the
correct frequency in 1 second:
PER = 3546895 / 880 = 4030 (PAL)
N.B.: The audio data is fetched from the DMA in 4 slots of the colour clock
(16bit=2 samples per channel) per horizontal scan line.
There are 312.5 PAL scan lines (312=SHF, 313=LOF) per
raster, and there are 50 rasters per second;
 therefore, the maximum read frequency (reading at all available assigned cycles)
 would be = 2 bytes per line * 312.5 * 50 = 31300 Hz
approximately: ** BUT THIS SPEED IS ONLY THEORETICAL **, as it is
impossible to set a correct sampling period that
coincides perfectly with the cycles assigned to the audio DMA at
each raster line: the end of a
period count from the DMA in the middle of a scan line
(or, if you're really unlucky, the cycle after the slot assigned
to the given voice), and the hardware is forced to wait for the
next line to get the data to play, while, if the sampling period 
is short, the next end of count
could occur in the same line, in the absence of new data.
In essence, the minimum period must allow the hardware
to run through AT LEAST one entire raster line: the sound does not
come out when it is read by the DMA, but when the count ends
from the period value in AUDxPER: the audio DMA
only reads data during the assigned cycles (which are
very high priority, like those of the disk drive DMA, to
avoid distortion and slowdowns due to channel ‘overcrowding’
- see: ‘bit planes that always break’) and stores them
in AUDxDAT until the end of the sampling period,
* which can occur at any time *, and the sound is generated
;
* for this reason, the minimum sampling period
cannot be lowered below 123 (=28836 Hz): to allow the DMA
to read at least one more piece of data before playing, on the
next line *.
At this point, the question is inevitable: ‘Why is 123
set as the minimum period when the clock cycles per line (i.e.
the number of decrements) are 226.5 (226=LOF, 227=SHF)?’ 
Here is the answer: it was mentioned earlier that the DMA
transfers 16 bits (=2 bytes) ‘at a time’ per voice, so
it has 2 playable samples per line, and, once the first one has been played,
it can also play the next one during the same
raster line, because it has not already read it; with 123,
in fact, at most 2 end-of-counts can occur during
the same line, and the problem does not arise.
The theoretical minimum period, therefore, should be 227 (to
be generous) / 2 = 114 (again, rounding up),
which more or less coincides (the correspondence between period and
sampling frequency is NOT biunivocal: there is always a
certain approximation) with the theoretical maximum frequency of
approximately 31300 Hz. However, as mentioned above, this cannot be
achieved with precision by the hardware.
*** 123 is the minimum period that can be set = 28836 Hz ***

AUDxVOL:    This register specifies the volume of the sound output
in the relevant channel with values between 0 and 64
(entering “64” does not result in any decrease in dB = maximum volume
).
*** TIP!!! When sampling, try to use the entire
range of values from -128 to 127, even for low-intensity sounds,
 in order to always have maximum precision, and,
at most, lower the volume in this register ***.

AUDxDAT:    This is the temporary buffer that the DMA uses before the data
are sent to the D/A (Digital/Analogue) converters and
the signals are emitted outside the Amiga.
It contains 2 bytes of audio data (the DMA transfers 16 bits
at a time from the RAM - this explains why
the AUDxLEN must be expressed in words!) which are sent
one by one to the DAC (Digital-Analogical Converter).
*** NOT RECOMMENDED!!! It is also possible to set these registers
with the CPU when the DMA is off and still make the computer play
:( ***.
_____ 
.__/_____\
\ O o /
/\ ____\_\_/___
\\\/___ : _ \
O\ \ / : / /
\\\_ : \/
_ __O\_/___:____\


- EXAMPLE of register settings to play a 23 kB sample
at a frequency of 21056 Hz located in memory at
location $60000 (RAM chip) at maximum volume in stereo,
with voices 2 and 3 (third and fourth channels):

PlaySample:
lea    $dff000,a0    ; base of custom chips in a0
move.l    #$60000,$c0(a0)    ; points AUD2LC to $60000
move.l    #$60000,$d0(a0)    ; also point AUD3LC to $60000
move.w    #11776,$c4(a0)    ; AUD2LEN = 23 kB = 23*1024 = 23552 B [...]
move.w	#11776,$d4(a0)    ; also AUD3LEN = [...] = 23552/2 = 11776 words
move.w    #168,$c6(a0)    ; AUD2PER = 3546895/21056 = 168
move.w    #168,$d6(a0)    ; sets AUD3PER as AUD2PER
move.w    #64,$c8(a0)    ; maximum volume for AUD2VOL
move.w    #64,$d8(a0)    ; maximum volume also for AUD3VOL
move.w    #$800c,$96(a0)	; turns on DMA for channels 2 and 3 in DMACON

Let's now explain what happens when the DMA for the channels is turned on (apart
from the fact that - obviously - you hear the sample...):

1 - The value contained in AUDLC is inserted into the
internal pointer registers and the DMA begins to transfer
2 bytes at a time to the data registers.
* From now on, the AUDLC register can also be changed:
as soon as the hardware has finished transferring the entire sample,
it will start again from the beginning (LOOPING INFINITELY).

2 - As soon as the AUDLC value is entered in the internal registers,
 a LEVEL 4 interrupt is triggered, which in the
INTENA and INTREQ registers, one
assigned to each of the 4 audio channels:

+--------------+---------------------+---------+
| IRQ LEVEL | INTENA/INTREQ BIT | CHANNEL |
+--------------+---------------------+---------+
| 4 |         10      | 3 |
| 4 |         9     | 2 |
| 4 |         8     | 1 |
| 4 |         7     | 0 |
+--------------+---------------------+---------+

Thanks to these interrupts, it is possible, for example, to
set a new sample to play as soon as the current one has finished
by simply pointing the location registers
to another sample, thus obtaining a perfect
connection between the two (provided that the two waveforms
end and begin in the same way).

3 - at the end of the transfer, everything starts again from point (1).
** The registers are never altered **.

.||||||.
\ oO ||
_\_-_/||_
/ {_{_ __ \
\ |____|/_/
/______\_)
___|_| |
/_/______|ck!^desejn

Sure, this is all interesting, you might say, but how can you
make a computer play a 10-minute song without sampling dozens of
megabytes of data?
This is precisely why trackers were invented: programmes designed to
write music that require only the basic instruments to be
sampled, obtaining the various notes by varying the reading frequency of
the same. They also feature an editor where you can compose the score divided into
4 tracks (one per voice), each of which can play any instrument at
any time, but always one at a time (a maximum of
4 instruments can be played simultaneously).
It would be very complicated to explain all the various possibilities of a
tracker here, so I invite you to get one (such as ProTracker: currently
the best 4-track tracker - yes, there are also multi-track trackers, which
mix the notes of multiple tracks in real time into a single voice; unfortunately,
however, this process is extremely slow and could not be used
to play the music of a demo or video game, since the machine, in
such cases, has much better things to do than waste all its time playing...).
The “philosophy” of trackers, however, remains the same: they all provide
so-called “music routines”, which are ASM sources that often use
level 6 interrupts (connected to the CIAB) or wait loops synchronised with the
electronic brush, and play the modules (song + samples =
score + instruments) created with the relevant tracker (or compatible ones, i.e.
those that save the module structure in the same way).
Adapting these routines to your own sources is very simple: in principle
- each one has its own conventions: read the .doc files for your tracker - it is
sufficient to launch an initialisation subroutine that sets the
interrupts and DMA channels - some also set the CIAB timers; the CIAA remains
intact as it is used by Exec to time processes/tasks -
and launch the play subroutine at each raster (it is best to do this at the beginning
of the vertical blank interrupt code - if you are under the operating system
add an interrupt server 5 (VBLANK, level 3) with high priority, to
do everything during the vertical blank interval, before the planes start
fetching and slowing everything down); before quitting your demo/game - or
program - REMEMBER to launch the restore subroutine for
interrupts, DMAs and timers to the OS.

Another important paragraph on Amiga audio hardware concerns the
modulation of the sound coming from the 4-voice DMA.
What is MODULATION? You have undoubtedly heard the effect of
AUDIO FADE (the gradual and slow decrease in volume) in many songs: well,
this simple effect is a particular type of modulation.
*** MODULATION consists of altering one or more parameters of a sound
during and beyond its period ***; the parameters in question are, of course,
INTENSITY (amplitude) and PITCH (frequency).
What effects do amplitude modulation and frequency modulation correspond to in terms of sound perception?
The former, as we have already mentioned, is commonly used in
fades, usually at the beginning and end of a piece of music; a
familiar example of frequency modulation is the slide on the strings of a
guitar (or a stringed instrument):
essentially, a subtle fusion of adjacent notes starting from a given frequency and arriving at another, passing gradually through all the intermediate frequencies with a certain speed (or even a certain acceleration). 
essentially, a smooth fusion of adjacent notes
starting from a given frequency and arriving at another, passing
gradually through all the intermediate frequencies at a certain speed (or even
a certain acceleration).
It is also possible to modulate both amplitude and frequency simultaneously,
obtaining a strange effect reminiscent of a phenomenon experienced in everyday life
: * the Doppler effect *.
In short, it consists of a change (modulation) in the intensity and
pitch of sounds coming from a source in relative motion with respect to
the listener: when you are walking down the street, you notice that the noise of
cars approaching and then passing you does not maintain the same
parameters in different positions of the car (source) with respect to you
(listener) but, first of all, it becomes louder in inverse proportion
to the distance between you and the source, and, if you pay close attention,
the intensity is not the only thing that changes over time: the frequency of the noise
emitted by the engine is also lower when the car is further away.
I don't think it's necessary to report the equation that describes the phenomenon
as a function of the speed of the two bodies and the distance, as the
problem does not closely relate to the topic of “modulation on Amiga”; an equation which,
however, can easily be found in any physics or general acoustics book,
 even at secondary school level.
Moving on to modulation on the Amiga, I am afraid I must disappoint you
right away: although the Paula has special operating modes for modulating
the sounds coming from a channel in both amplitude and frequency, this hardware solution is never used
because it has a terrible
restriction: to modulate intensity and pitch, the DMA must read from the RAM
the values to be inserted in the AUDxVOL and/or AUDxPER registers, while
another DMA reads the actual values of the sample to be played and then
distorted; this process has the limitation that the DMA that reads from the
modulation value table must be one of the audio channels, so to
modulate, for example, the sound read from channel 0 in both frequency and
amplitude, we are forced to use channels 1 and 2 to read the respective
tables, resulting in wasting 3 channels to generate a single modulated sound.
All modulation effects used by trackers are managed by the CPU,
which “maliciously” sets the volume and period registers of the desired voice
while the DMA reads its sample unaware. In this way, no channel is wasted, even if the CPU remains busy for a while calculating
the sound effects in real time.
The Amiga does not even have an FM synthesiser
 (Frequency Modulation) capable of creating different timbres from the same waveform.
These apply both amplitude and frequency modulations according to 4
parameters called ADSR, from the initials of the 4 main phases of a synthesised sound
: Attack, Decay, Sustain, Release.
The graph of this modulation is as follows:

b
/\
/ \ D
/ \ S
A / \___________d
/ c \
/         \ R
/         \
_ _ _ ______________/                \____________________ _ _ _
a                 e

The first phase is the Attack, which consists of bringing the volume and/or frequency
of the wave from “à to 'b” (i.e. from 0 to a maximum peak value); after that,
the graph descends along the Decay section to a level “c”, where it
stabilises for the duration of the Sustain; finally, it returns to 0 from “d” to 'è with the
Release.
** By “playing” cleverly with the position of points “à,” 'b,“ 'c,” 'd' and “è” and
with the durations of the various phases, it is possible to generate an infinite number of sounds
even starting from the sample of a simple harmonic **.
Unfortunately, these notions will not be useful for programming the Amiga sound chip,
 so let's move on to the description of those bits that were never
set in the history of Amiga and its hardware... :)
The register in question is the infamous ADKCON ($dff09e), which also has
a read copy (ADKCONR) at address $dff010:

bit - 7: USE3PN Use channel 3 to modulate nothing
6: USE2P3 Use channel 2 to modulate the PERIOD of 3
5: USE1P2 Use channel 1 to modulate the PERIOD of 2
4: USE0P1 Use channel 0 to modulate the PERIOD of 1
		 
3: USE3VN Use channel 3 to modulate nothing
2: USE2V3 Use channel 2 to modulate the VOLUME of 3
1: USE1V2 Use channel 1 to modulate the VOLUME of 2
0: USE0V1 Use channel 0 to modulate the VOLUME of 1
		 

You will have understood how this works: if, for example, you need to
modulate the amplitude of channel 2, you can only do so by using 1 as the
reader of the values to be entered in the AUD2VOL register, so you will need to point the
channels to the relevant data and give them a reading frequency.
** Modulation, however, is a simple but important effect, which must
be simulated via the CPU - as already mentioned - so as not to occupy any of
the Amiga's already few audio channels... **

O .... o 
o :¦ll¦: 
___( “øo` )___ 
/¨¨¨(_ `____)¨¨¨\
(__, `----U-” .__)
( ¬\\_>FATAL< _,/¯ )
(__)\ ¯¯‘:’¯¯¯ /(__)
(,,) \__ : ___/ (,,)
(_\¯¯¯ /_)
:...( Y ¬) ··:
_\___|____/_
`-----`------'

I would conclude by saying that knowledge of digital acoustics and
the functioning of sound chips in general is not as essential as that
of graphics hardware or CPU assembly language, and it is certain that mastery of
these is necessary for everyone, including audiophiles; it is equally true,
however, that true knowledge of the subject also requires an in-depth understanding
of digital sound theory, which is so highly praised these days
but is largely ignored by most people, who
think just like those “coders” who snub - not to say “skip” -
completely ignore sources of information on the subject because “
music routines just need to be called from the interrupt...”.

****************************************************************************
* PART 2: SOPHISTICATED REPLAY ROUTINES (author: Fabio Ciucci)     *
****************************************************************************

Regarding these music routines, so far we have only seen the standard one
provided with the protracker, but there are also more sophisticated ones.
We will now look at one of the best, player6.1a, which also requires a
conversion programme (p61con, on this disc), with which we must transform a
normal module into one optimised for the replay routine.

NOTE: This player is copyright of the author:

Jarno Paananen / Guru of Sahara Surfers.
ÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂ

J.Paananen
Puskalantie 6
FIN-37120 Nokia
Finland

Internet:     Jarno_Paananen@sonata.fipnet.fi
jpaana@freenet.hut.fi


Therefore, if you use his replay routine in a commercial product,
for example in a game, you must obtain his written permission and give him
something (in Finnish marks!) as a percentage.
He'll be mad at me for not including the whole archive!!!...

This player has many options, but let's start with the simplest
: playing a module as we did with the standard routine provided
with the protracker programme.

Here's what you need to do:

1) Convert the module to P61 format using the ‘P61CON’ utility. This
utility requires reqtools.library and powerpacker.library in the
libs directory to work. In the programme preferences, don't
touch anything, leaving only the ‘time’ option set.
When saving, make a note of the USECODE, which will need to be specified in the listing
at the ‘use = ....’ equate. This is to save code.

2) This will give you the converted, UNCOMPRESSED module.
However, the module is often shortened due to the optimisation that
is performed automatically.

3) Now just do as with the previous routines: call P61_Init before
playing, then P61_Music at each frame, and P61_End at the end.
The only additional requirement is to enable level 6 interrupts.

Let's look at a practical example in Lesson 14-10a.s

You will notice that it is faster than the standard one, but also that it uses the
A timer of the CIAB and the level 6 interrupt ($78).
There are also some equates, whose meaning you need to know:

fade = 0    ;0 = Normal, NO master volume control possible
;1 = Use master volume (P61_Master)

This must be set to 1 if you want to control the volume to create a fade,
acting on the P61_Master label. We will see an example later. If we do not need this
option, we set it to 0 to save code, as these equates
are nothing more than conditional assemblies that use the
assembler directives ‘ifeq’, “ifne”, ‘endc’...

jump = 0    ;0 = do NOT include position jump code (P61_SetPosition)
;1 = Include

This option should also be left at zero if you are not using the jump routine
to a specific position in the module. We will see an example later.

system = 0    ;0 = killer
;1 = friendly

It is best to leave this option at 0 if you are writing ‘malicious’ code and using
startup2.s. Just be careful when loading from DOS!!! (You must also
leave the $78 (level 6) interrupt in place, without restoring the
system one, in case you load with this replay routine active!).

CIA = 0        ;0 = CIA disabled
;1 = CIA enabled

This option should be kept at 0 to use the routine in ‘standard’ mode. If you
set it to 1, you will no longer need to call P61_Music at every frame, because
the timing will be done entirely by the CIAB. We will see an example.

exec = 1    ;0 = ExecBase destroyed
;1 = ExecBase valid

This should be left at 1, since we leave the execbase valid in $4.w... we're not
maniacs! Why do we need the startup?

opt020 = 0    ;0 = MC680x0 code
;1 = MC68020+ or better

This is clear: if your game/demo is AGA only, you can set this
to 1, otherwise leave it at zero. Be careful not to set it to 1 when it is not
needed!!!!!! ONLY IF THE GAME/DEMO RUNS **ONLY** ON AGA (i.e. 68020+).

use = $2009559    ; Usecode (set the value given by p61con when saving
; different for each module!)

The comment here explains everything... always write down the usecode on a piece of paper
(and don't lose the piece of paper, of course), and put it here.
This is needed to assemble only the routines of the effects used in the module,
saving space. Setting -1 means assembling everything (yuck!).
____ 
/ \
_ |______| _
/(_|/\/\/\|_)\
(______________)
| .. |
| \__/ |
|______|
.--' `--.
| | | |
| |______| |
|_||||||||_|
(^) ____ (^)
| || |
_| _||_ |_
/____\/____\

The conversion programme also allows you to compress the module, but
this causes some loss of quality. So I don't recommend compressing them...
Unless you have to make a 40k intro and you're really pressed for space,
 it's always best to use the ‘normally’ converted module.
However, the program allows you to choose which samples to compress and which
not... and to listen with your own ears if too much quality is lost!

Here's what you need to do to compress and replay a compressed module:

1) Convert the module to compressed P61 format using “P61CON”.
In the programme preferences, you need to enable the “pack samples” option.
Note that you can choose which samples to compress and which not to.
Here's what you'll see for each sample:

Original     -Plays the original sample (Stop with right mouse button)
Packed     -Plays the sample as it would be compressed. If you notice that
too much quality is lost, think again...!
Pack         -Mark this sample as ‘to be compressed’
Pack rest     -Compress all other samples from here onwards
Don't pack     -Do not compress this sample
Don't pack rest -Do not compress all other samples (from here onwards)
As always, make a note of the USECODE when saving.
In addition, make a note of the ‘sample buffer length’!!!!!!

2) In this way, we have obtained the converted and COMPRESSED module.

3) Now there are two more things to do: first of all, the module has compressed samples,
 which must be decompressed into a buffer. To do this, you need to do
two things: create the buffer, as long as indicated by the program as ‘sample buffer
length’, and put its address in a2 before calling P61_Init, which
will decompress it. The rest (play and end) is the same.
Let's see how it works in practice:

movem.l    d0-d7/a0-a6,-(SP)
lea    P61_data,a0    ; Module address in a0
lea    $dff000,a6    ; Remember $dff000 in a6!
sub.l    a1,a1        ; The samples are not separate, let's put zero
*******************
>>>>>	lea    samples,a2    ; Compacted module! Destination buffer for
*******************        ; the samples (in chip RAM) to be indicated!
bsr.w    P61_Init    ; Note: it takes a few seconds to decompress!
movem.l    (SP)+,d0-d7/a0-a6

For the module and buffer, here are the changes:

1) The module no longer needs to be loaded into chip RAM:

Section    modulozzo,data    ; It does not need to be in chip RAM because it is
; compressed and will be decompressed elsewhere!
P61_data:
incbin    ‘P61.stardust’    ; Compressed (PACK SAMPLES option)

2) The buffer must be loaded into CHIP RAM, and be as long as specified:

section    smp,bss_c

samples:
ds.b    132112    ; length reported by p61con


As you will notice, in addition to losing quality in the samples, more memory is also used,
as we have the buffer, even though the module is shorter.

Let's look at an example in Lesson 14-10b.s

Now that we have seen the two main implementations, we can look at all
the variants. First of all, the CIA option, which is enabled with the equate.
You can see two examples in Lesson 14-10c.s and Lesson 14-10d.s

or
or. ______
°O|.____.|
°o.|| .. ||
O|`----“|
|______|
.--” `--.
| | | |
| | | |
|_|______|_|
(^) ____ (^)
| || |
_| _||_ |
/____\/___\

Finally, let's look at the use of two optional features:

Audio fade: just activate the ‘fade’ equate and use the appropriate
label ‘P61_Master’, which ranges from 0 to 64. Example in Lesson 14-10e.s

The ability to jump to arbitrary positions in the module: just activate
the ‘jump’ equate and call the ‘P61_SetPosition’ routine, with the position
in register d0. Example in Lesson 14-10f.s

There are also other optional features, which we can summarise in the preferences:

Two files:		This option saves the samples and the
song separately in two files. This can be useful if we use multiple modules
with the same samples...

P61A sign:        puts the P61A sign at the beginning of the module... this can
only be used to make it easier for malicious users to
rip it!!! Never set it!

No samples:        Useful when saving many modules that have the
same samples: the first time, set ‘two files’
and save the modules and the first song. Then set
this option and save all the other songs.

Tempo:            To enable the ‘Tempo’ option in the player.

Icon:            If you want to save an icon together with the module

Delta:            8-bit compression instead of 4-bit (I noticed
that it doesn't change much... oh well!)

Sample packing:        To be set for sample compression with
the 4-bit delta algorithm (LOSS OF QUALITY!!!).

____ ________ ____
\__ \__ / __ \ __/ __/
\____|o o \/ o o|____/
\__|__________|__/
| ___/__\
_|__ \__/ \\_
(_______/U \/
____/\_\___U_/ \_____/_/\____
\ ___/ (_(_______)_) \___ /
\_\/ / | \ \/_/
/ / | \ \
/ /________|_______/\ \
/ / _____ \ \
\ / /
\_______________________/

Happy listening, everyone!
